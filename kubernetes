service can be accessed only with in pod

Each Pod in Kubernetes gets a unique IP address managed by the CNI (Container Network Interface) plugin. This unique IP addressing scheme allows for 
direct communication between Pods, simplifies service discovery, and ensures network isolation. The CNI plugin and its IPAM component 
handle the assignment and management of these IP addresses, adhering to the Kubernetes networking model

network namespace can be found under var/run/netns 

Yes, the Container Runtime Interface (CRI) plugin  create a network namespace

when we not sepcify any namespace its used default namespace 

within namespace object name should be unique 

we can use same object name for diffrent namespaces

we have to carefully give selectors and labels for the specific pod to access (or) else we get error 

to enter inside container from pod we use --> kubectl exec -it podname -n namespacename-ns -- sh
in  the above if we have more continers in the pod it will exec by index value 0th index first

if we have more contianers in the pod we use contianer name -->kubectl exec -it podname -c contianername -n namespacename-ns  --sh

we communicate b/w pods within node by using localnetwork (or) pod ip 

service will act as a loadbalancer and service discovery 
 *cluster port we can access within cluster only --> container we can access internally only 
 *nodeport we can access from outside cluster webpage 30000 to 32767 node port range --> container we can access externally like webpages by giving port no

if we have 2 pods first pod will be executed if we execute pod 

if we want to execute specific container in a pod we have to give container name 
eg - kubectl exec -it podname -c containername bash 

curl -v servicename/containername --> communicates with a web (or) application serverby specifying a relevant URL and the data that need to be sent or received 

imp:
if 2 pods are in same namespace we communicate by using fqdn (FULLY QULIFIED DOMAIN NAME)
eg: curl -v servicename.namespacename.svc.cluster.local/contianerrname 
curl -v mavenwebappsvc.test-ns.svc.cluster.local/mavenwebapplication 

Multicontainer pod.yml
we have to give different port numbers so we can use multicontainers
ports:
  - port: 8080
  - targetport: 8080
    name: tomcat
  - port: 80
  - targetport: 80
    name: nginix

if we have different namespace and we should access pods of different namespaces 

NAMESPACE
---------
https://kubernetes.io/docs/concepts/workloads/pods/user-namespaces/

User namespaces is a Linux feature that allows to map users in the container to different users in the host. Furthermore, the capabilities granted to a pod in a user namespace are valid only in the namespace and void outside of it.
The kubelet will pick host UIDs/GIDs a pod is mapped to, and will do so in a way to guarantee that no two pods on the same node use the same mapping.
The valid UIDs/GIDs when this feature is enabled is the range 0-65535. This applies to files and processes (runAsUser, runAsGroup, etc.)
When creating a pod, by default, several new namespaces are used for isolation: a network namespace to isolate the network of the container, a PID namespace to isolate the view of processes, etc. If a user namespace is used, this will isolate the users in the container from the users in the node.

This means containers can run as root and be mapped to a non-root user on the host. Inside the container the process will think it is running as root (and therefore tools like apt, yum, etc. work fine), while in reality the process doesn't have privileges on the host. You can verify this, for example, if you check which user the container process is running by executing ps aux from the host. The user ps shows is not the same as the user you see if you execute inside the container the command id.

Furthermore, as users on each pod will be mapped to different non-overlapping users in the host, it is limited what they can do to other pods too.


PODLIFECYCLE
-----------
https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/

Pods follow a defined lifecycle, starting in the Pending phase, moving through Running if at least one of its primary containers starts OK, and then through either the Succeeded or Failed phases depending on whether any container in the Pod terminated in failure.

Whilst a Pod is running, the kubelet is able to restart containers to handle some kind of faults. Within a Pod, Kubernetes tracks different container states and determines what action to take to make the Pod healthy again


DEPLOYMNENTS
------------
https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.



CONTROLL PROBES
---------------
https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/

probe means health check 

Types of probe
--------------
The kubelet can optionally perform and react to three kinds of probes on running containers:

livenessProbe
--------------
Indicates whether the container is running. If the liveness probe fails, the kubelet kills the container, and the container is subjected to its restart policy. If a container does not provide a liveness probe, the default state is Success.

2 types we have (Define a liveness command  / Define a liveness HTTP request)
--> Define a liveness command 
 livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5

kubectl get pod liveness-exec 

--> Define a liveness HTTP request
Another kind of liveness probe uses an HTTP GET request 

livenessProbe:
      httpGet:
        path: /healthz
        port: 8080
initialDelaySeconds: 5
periodSeconds: 5

kubectl describe pod liveness-http  --> to get the http request liveness

Any code greater than or equal to 200 and less than 400 indicates success. Any other code indicates failure.

--> Define a TCP liveness probeDefine a TCP liveness probe
A third type of liveness probe uses a TCP socket. With this configuration, the kubelet will attempt to open a socket to your container on the specified port. If it can establish a connection, the container is considered healthy, if it can't it is considered a failure. 

readinessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 10
    livenessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 10

--> Use a named port
You can use a named port for HTTP and TCP probes. gRPC probes do not support named ports.

For example:

ports:
- name: liveness-port
  containerPort: 8080

livenessProbe:
  httpGet:
    path: /healthz
    port: liveness-port

readinessProbe
---------------
Indicates whether the container is ready to respond to requests. If the readiness probe fails, the endpoints controller removes the Pod's IP address from the endpoints of all Services that match the Pod. The default state of readiness before the initial delay is Failure. If a container does not provide a readiness probe, the default state is Success.

 ReadynessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
The periodSeconds field specifies that the kubelet should perform a liveness probe every 5 seconds. The initialDelaySeconds field tells the kubelet that it should wait 5 seconds before performing the first probe. To perform a probe, the kubelet executes the command cat /tmp/healthy in the target container. If the command succeeds, it returns 0, and the kubelet considers the container to be alive and healthy. If the command returns a non-zero value, the kubelet kills the container and restarts it.

startupProbe
-------------
Indicates whether the application within the container is started. All other probes are disabled if a startup probe is provided, until it succeeds. If the startup probe fails, the kubelet kills the container, and the container is subjected to its restart policy. If a container does not provide a startup probe, the default state is Success.

HPA
----
https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/

https://kubernetes.io/docs/tutorials/kubernetes-basics/scale/scale-intro/

A HorizontalPodAutoscaler (HPA for short) automatically updates a workload resource (such as a Deployment or StatefulSet), with the aim of automatically scaling the workload to match demand.

Horizontal scaling means that the response to increased load is to deploy more Pods. This is different from vertical scaling, which for Kubernetes would mean assigning more resources (for example: memory or CPU)
to the Pods that are already running for the workload.

If the load decreases, and the number of Pods is above the configured minimum, the HorizontalPodAutoscaler instructs the workload resource (the Deployment, StatefulSet, or other similar resource) to scale back down.

The HPA controller will increase and decrease the number of replicas (by updating the Deployment) to maintain an average CPU utilization across all Pods of 50%. The Deployment then updates the ReplicaSet 
- this is part of how all Deployments work in Kubernetes - and then the ReplicaSet either adds or removes Pods based on the change to its .spec.

kubernetes we will create normally depolyment and attach HPA  to the depolyment we will use HPA scaletargetref (kind , name) to the deployment section same kind and name should match then only HPA will 
work as expected
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10

runtime give [kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10]

declarative giving 

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50


volumes
-------
https://kubernetes.io/docs/concepts/storage/volumes/

On-disk files in a container are ephemeral (lasting for a very short time)
which presents some problems for non-trivial applications when running in containers. One problem occurs when a container crashes or is stopped. Container state is not saved so all of the files that were created or modified during the lifetime of the container are lost. During a crash, kubelet restarts the container with a clean state. Another problem occurs when multiple containers are running in a Pod and need to share files. It can be challenging to setup and access a shared filesystem across all of the containers. The Kubernetes volume abstraction solves both of these problems.

Volumes cannot mount within other volumes (but see Using subPath for a related mechanism). Also, a volume cannot contain a hard link to anything in a different volume

Kubernetes supports many types of volumes. A Pod can use any number of volume types simultaneously. Ephemeral volume types have a lifetime of a pod, but persistent volumes exist beyond the lifetime of a pod. 

volumes:
  - name: example-volume
    # mount /data/foo, but only if that directory already exists
    hostPath:
      path: /data/foo # directory location on host
      type: Directory # this field is optional
TYPES OF VOLUMES
----------------
Kubernetes supports several types of volumes. 
Like
emptydir -->
--------
For a Pod that defines an emptyDir volume, the volume is created when the Pod is assigned to a node. As the name says, the emptyDir volume is initially empty. All containers in the Pod can read and write the same files in the emptyDir volume, though that volume can be mounted at the same or different paths in each container. When a Pod is removed from a node for any reason, the data in the emptyDir is deleted permanently.

eg:
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: registry.k8s.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir:
      sizeLimit: 500Mi

configmap 
hostpath-->
--------
A hostPath volume mounts a file or directory from the host node's filesystem into your Pod. This is not something that most Pods will need, but it offers a powerful escape hatch for some applications.
Warning:
Using the hostPath volume type presents many security risks. If you can avoid using a hostPath volume, you should. For example, define a local PersistentVolume, and use that instead.
Take care when using hostPath volumes, whether these are mounted as read-only or as read-write, because:

Access to the host filesystem can expose privileged system credentials (such as for the kubelet) or privileged APIs (such as the container runtime socket), that can be used for container escape or to attack other parts of the cluster.
Pods with identical configuration (such as created from a PodTemplate) may behave differently on different nodes due to different files on the nodes.

Some uses for a hostPath are:
running a container that needs access to node-level system components (such as a container that transfers system logs to a central location, accessing those logs using a read-only mount of /var/log)
making a configuration file stored on the host system available read-only to a static pod; unlike normal Pods, static Pods cannot access ConfigMaps

---
# This manifest mounts /data/foo on the host as /foo inside the
# single container that runs within the hostpath-example-linux Pod.
# The mount into the container is read-only.
apiVersion: v1
kind: Pod
metadata:
  name: hostpath-example-linux
spec:
  os: { name: linux } 
  nodeSelector:
    kubernetes.io/os: linux
  containers:
  - name: example-container
    image: registry.k8s.io/test-webserver
    volumeMounts:
    - mountPath: /foo
      name: example-volume
      readOnly: true
  volumes:
  - name: example-volume
    # mount /data/foo, but only if that directory already exists
    hostPath:
      path: /data/foo # directory location on host
      type: Directory # this field is optional


Local --> A local volume represents a mounted local storage device such as a disk, partition or directory.
------
Local volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported.

Compared to hostPath volumes, local volumes are used in a durable and portable manner without manually scheduling pods to nodes. The system is aware of the volume's node constraints by looking at the node affinity on the PersistentVolume.

However, local volumes are subject to the availability of the underlying node and are not suitable for all applications. If a node becomes unhealthy, then the local volume becomes inaccessible by the pod. The pod using this volume is unable to run. Applications using local volumes must be able to tolerate this reduced availability, as well as potential data loss, depending on the durability characteristics of the underlying disk.

NFS
----
An nfs volume allows an existing NFS (Network File System) share to be mounted into a Pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of an nfs volume are preserved and the volume is merely unmounted. This means that an NFS volume can be pre-populated with data, and that data can be shared between pods. NFS can be mounted by multiple writers simultaneously.

Note:
You must have your own NFS server running with the share exported before you can use it.

Also note that you can't specify NFS mount options in a Pod spec. You can either set mount options server-side or use /etc/nfsmount.conf. You can also mount NFS volumes via PersistentVolumes which do allow you to set mount options.

eg:
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: registry.k8s.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /my-nfs-data
      name: test-volume
  volumes:
  - name: test-volume
    nfs:
      server: my-nfs-server.example.com
      path: /my-nfs-volume
      readOnly: true

secret
-------
A secret volume is used to pass sensitive information, such as passwords, to Pods. You can store secrets in the Kubernetes API and mount them as files for use by pods without coupling to Kubernetes directly. secret volumes are backed by tmpfs (a RAM-backed filesystem) so they are never written to non-volatile storage.

azurefilesystem
cephfs

presistent - volumes
--------------------
https://kubernetes.io/docs/concepts/storage/persistent-volumes/

A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. It is a resource in the cluster just like a node is a cluster resource. PVs are volume plugins like Volumes

A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany, ReadWriteMany, or ReadWriteOncePod)

Provisioning
-------------
There are two ways PVs may be provisioned: statically or dynamically.

//Static
A cluster administrator creates a number of PVs. They carry the details of the real storage, which is available for use by cluster users. They exist in the Kubernetes API and are available for consumption.

//Dynamic
When none of the static PVs the administrator created match a user's PersistentVolumeClaim, the cluster may try to dynamically provision a volume specially for the PVC. This provisioning is based on StorageClasses: the PVC must request a storage class and the administrator must have created and configured that class for dynamic provisioning to occur. Claims that request the class "" effectively disable dynamic provisioning for themselves.

To enable dynamic storage provisioning based on storage class, the cluster administrator needs to enable the DefaultStorageClass admission controller on the API server. This can be done, for example, by ensuring that DefaultStorageClass is among the comma-delimited, ordered list of values for the --enable-admission-plugins flag of the API server component. For more information on API server command-line flags, check kube-apiserver documentation.

//Binding
A user creates, or in the case of dynamic provisioning, has already created, a PersistentVolumeClaim with a specific amount of storage requested and with certain access modes. A control loop in the control plane watches for new PVCs, finds a matching PV (if possible), and binds them together. If a PV was dynamically provisioned for a new PVC, the loop will always bind that PV to the PVC. Otherwise, the user will always get at least what they asked for, but the volume may be in excess of what was requested. Once bound, PersistentVolumeClaim binds are exclusive, regardless of how they were bound. A PVC to PV binding is a one-to-one mapping, using a ClaimRef which is a bi-directional binding between the PersistentVolume and the PersistentVolumeClaim.

Claims will remain unbound indefinitely if a matching volume does not exist. Claims will be bound as matching volumes become available. For example, a cluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi. The PVC can be bound when a 100Gi PV is added to the cluster.

Config Maps
------------
A ConfigMap provides a way to inject configuration data into pods. The data stored in a ConfigMap can be referenced in a volume of type configMap and then consumed by containerized applications running in a pod.

When referencing a ConfigMap, you provide the name of the ConfigMap in the volume. You can customize the path to use for a specific entry in the ConfigMap. The following configuration shows how to mount the log-config ConfigMap onto a Pod called configmap-pod:

apiVersion: v1
kind: Pod
metadata:
  name: configmap-pod
spec:
  containers:
    - name: test
      image: busybox:1.28
      command: ['sh', '-c', 'echo "The app is running!" && tail -f /dev/null']
      volumeMounts:
        - name: config-vol
          mountPath: /etc/config
  volumes:
    - name: config-vol
      configMap:
        name: log-config
        items:
          - key: log_level
            path: log_level

The log-config ConfigMap is mounted as a volume, and all contents stored in its log_level entry are mounted into the Pod at path /etc/config/log_level. Note that this path is derived from the volume's mountPath and the path keyed with log_level.

kubectl get events -n namespacename 
to see events for the specific namespace like logs

livenessprobe and readynessprobe we have to use mandatory to know the application status running  and  responding to recive the traffic for that application 

Pull an Image from a Private Registry
--------------------------------------
refer offical link
https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ 

 create a Pod that uses a Secret to pull an image from a private container image registry or repository. There are many private registries in use
https://index.docker.io/v1/  

Create this Secret, naming it regcred:

we should add in container specifications field it will be applicable for only that namespace 
 imagePullSecrets:
  - name: regcred

kubectl create secret docker-registry regcred --docker-server=<your-registry-server> --docker-username=<your-name> --docker-password=<your-pword> --docker-email=<your-email>

where:
<your-registry-server> is your Private Docker Registry FQDN. Use https://index.docker.io/v1/ for DockerHub.
<your-name> is your Docker username.
<your-pword> is your Docker password.
<your-email> is your Docker email.
You have successfully set your Docker credentials in the cluster as a Secret called regcred.

To get the output from the secret
kubectl get secret regcred --output=yaml  

Note: To use image pull secrets for a Pod (or a Deployment, or other object that has a pod template that you are using), you need to make sure that the appropriate Secret does exist in the right namespace. The namespace to use is the same namespace where you defined the Pod.

Another private registry EKS 
AWS EKS

we will create a IAM role and assign policy for that ec2 instance Readonlypolicy assign and give 
we can see now based on policy that server will autoamtocally fetch the private image from the eks registry and we will not require any secret to use for the authentication wile comes to AWS cloud

stateful sets 
-------------
"A StatefulSet is a Kubernetes controller that manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods. It's particularly useful for applications that require stable, unique network identities and storage for each of their components.

StatefulSets are typically used for stateful applications, such as databases, where each instance needs a stable, unique identity and the ability to handle persistent data. They are designed to maintain the identity of the Pods, even when they are rescheduled, and to ensure that each Pod has a unique hostname and DNS name.

Here are some key features of StatefulSets:

Stable, unique network identities: Each Pod in a StatefulSet has a unique, stable hostname and DNS name, allowing other services to reliably discover and connect to them.

Persistent storage: StatefulSets support the use of Persistent Volumes (PVs) and Persistent Volume Claims (PVCs) to provide each Pod with stable, persistent storage.

Ordered deployment and scaling: StatefulSets deploy and scale their Pods in a specific order, ensuring that each Pod is fully up and running before the next one is started.

Ordered deletion: When a StatefulSet is deleted, its Pods are terminated in reverse order, ensuring that each Pod has a chance to cleanly shut down and release any resources it may be holding.

Rolling updates: StatefulSets support rolling updates, allowing you to update the Pods in a controlled, predictable manner.

To use a StatefulSet, you define a StatefulSet object in your Kubernetes cluster, specifying the number of replicas, the container image, and any other necessary configuration options. The Kubernetes control plane will then create and manage the Pods according to your specifications.

In summary, StatefulSets are a powerful tool for deploying and managing stateful applications in Kubernetes. By providing stable, unique network identities and persistent storage for each Pod, they enable you to run complex, stateful applications with confidence and ease."

**To change namespace of the cluster we can use below command
kubectl config set-contxt --current --namespace=test-ns
kubectl config view
  we can see that clusternamespace is changed to test-ns
kubectl config set-contxt --current --namespace=default
  we can see that clusternamespace is changed to default

kubectl get pods --kubeconfig ~./kube/config 
o/p --> norespurce found in default namespace

kubectl get pods --kubeconfig ~./kube/config -n test-ns
o/p --> display all resurces

kubectl get nodes --show-labels --> display default labels of the nodes 

scheduling 
----------
nodeselector --> when we give node selector it will schedule specfic node on matching node selector 
kubectl label node <nodename/nodeip> <lebel-key>=<label-value>
kubectl label node ip:172.52.185.4 name=workerone storage=ssd

now we can use this node label as a node selector

in any pod or ds or rs use in specification section
spec:
  nodeselector:
    name=workerone
it will assign to node which have workerone only when it matches 
if we have taints on the node node wont schedule it wil be in waiting state
but here that one node which has label went down pods wont scheduled to remaining nodes here' node affinifty comes into picture

Node Affinity
--------------
nodeaffinifty will schedule pods on any node 
we have 2 rules
1.hardrule (wont schedule on unmatched nodes )
2.softrule ( schedule on unmatched nodes )
cardon / uncardon/taints/tolerations/pod affinity , nod, pod antiaffinity

managed kubernetes cluster 
---------------------------------
EKS ,GKS ,AKS,etc..
EKS we use in aws 
1.create a eks cluster
2 . create a IAM role and give required permission 
3. create a node group


KOP's --> by using k8s operations softeare we can setup high availble and production ready k8s cluster in aws (support others like azure )
kops will configure one launch cofig , and one autoscaling group for master
another one launch config , and one autoscaling group for workers

ingress controlller
--------------------
https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/

Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.

Ingress resource to work, the cluster must have an ingress controller running.

Unlike other types of controllers which run as part of the kube-controller-manager binary, Ingress controllers are not started automatically with a cluster.
https://www.nginx.com/blog/deploying-nginx-ingress-controller-on-amazon-eks-how-we-tested/ 

An Ingress may be configured to give Services externally-reachable URLs, load balance traffic, terminate SSL / TLS, and offer name-based virtual hosting. An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic.

An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically 

we are havong 2 types of ingrress controllers to route the traffic outside world 
1 hostbased routing
2 pathbased routing
1 hostbased used to give as comon host and path can be anything 
2 pathbased have differnt hostpaths

# hostbased routing
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wildcard-host 
spec:
  rules:
  - host: "foo.bar.com" --> hostpath of the website we give here
    http:
      paths:
      - pathType: Prefix
        path: "/" --> after / is the subpath can be any thing in hostbased routing
        backend:
          service:
            name: service
            port:
              number: 80
# pathbased routing
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wildcard-host 
spec:
  rules:
  - host: "foo.bar.com" --> hostpath of the website we give here
    http:
      paths:
      - pathType: Prefix
        path: "/api/tasks" --> after / is the pathbased routing
        path: "/java-webapp/" --> nth path
        backend:
          service:
            name: service
            port:
              number: 80


** Ingress class
Ingresses can be implemented by different controllers, often with different configuration. Each Ingress should specify a class, a reference to an IngressClass resource that contains additional configuration including the name of the controller that should implement the class.

apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  name: external-lb
spec:
  controller: example.com/ingress-controller
  parameters:
    apiGroup: k8s.example.com
    kind: IngressParameters
    name: external-lb

RBAC 
---
// refer RBAC pdf 
RBAC Repository 
rbac use for accessing only spcific user and group permissions
rbac use 3 key concepts VERBS , API RESOURCE, SUBJECTS
role 
role binding cluster rolebinding 

Create Role:
-----------
kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
name: devrole
namespace: default
rules:
- apiGroups: [""]
resources: ["pods"]
verbs: ["get", "update", "list"] --> //user have specific access to pods like get ,, update ,list 


Create RoleBinding :
------------------
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
name: dev-role-binding-user
namespace: default
subjects:
- kind: User
name: Michale
apiGroup: rbac.authorization.k8s.io
roleRef:
kind: Role
name: devrole
apiGroup: rbac.authorization.k8s.io

HELM
-----
Helm is the package manager for Kubernetes. It allows you to
install/deploy/remove/update applications on your Kubernetes cluster in a
similar manner to yum/apt for Linux distributions.

* Why use Helm?
Kubernetes can be difficult to manage with all the objects you need to maintain
(ConfigMaps/Secrets, pods, services, etc.). Helm manages all of this for you. Helm greatly simplifies the process of creating, managing, and deploying
applications using Helm Charts.

* What is a Helm Chart?
In Helm, a chart is basically just a collection of manifest files organized in a
specific directory structure that describe a related Kubernetes resource. There
are two main components to a chart: templates and values. These templates and
values go through a template rendering engine, producing a manifest that is
easily digestible by Kubernetes.
Helm uses Charts to pack all the required K8S components(Manifests) for an
application to deploy, run and scale.

Helm concepts | Charts
=========================
Helm packages are called charts, and they consist of a few YAML
configuration files and some templates that are rendered into Kubernetes
manifest files.


Here is the basic directory structure of a chart:

javawebapp
├── Chart.yaml # Meta data information about chart.
├── charts # define dependent helm chart names
├── templates # Will maintain k8’s manifest files which is required for our application
│ ├── deployment.yaml
│ ├── hpa.yaml
│ ├── ingress.yaml
│ ├── service.yaml
│ ├── serviceaccount.yaml
│
└── values.yaml # Will define default values which will be referred in templates.

These directories and files have the following functions:
 charts/: Manually managed chart dependencies can be placed in this
directory, though it is typically better to use requirements.yaml to
dynamically link dependencies.


No more maintaining random groups of YAML files (or very long ones)
describing pods, replica sets, services, RBAC settings, etc. With helm, there is a
structure and a convention for a software package that defines a layer of
YAML templates and another layer that changes the templates
called values. Values are injected into templates, thus allowing a separation of
configuration, and defines where changes are allowed. This whole package is
called a Helm Chart.
Essentially you create structured application packages that contain everything
they need to run on a Kubernetes cluster; including dependencies the
application requires

Monitoring tool
---------------
Promethus
---------
* What Is Prometheus Monitoring?
Prometheus is an open source system that collects and manages server and application metrics. It
can also be configured to notify your team when an issue arises.

* Prometheus is an opensource monitoring and alerting toolkit. Prometheus consists of several
components some of which are listed below:
 The Prometheus server which scrapes(collects) and stores time series data based on a pull
mechanism.
 A rules engine which allows generation of Alerts based on the scraped metrices.
 An alertmanager for handling alerts.
 Multiple integrations for graphing and dashboarding.

Enterprises looking to decrease downtime and optimize resources can implement server
monitoring using tooThe Prometheus monitoring system includes a rich, multidimensional data model, a concise and
powerful query language called PromQL, an efficient embedded timeseries database.

Alert Manager:
The Alertmanager handles alerts sent by client applications such as the Prometheus server. It
takes care of deduplicating, grouping, and routing them to the correct receiver integration such as
email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts.

Node Exporter:
The Prometheus Node Exporter exposes a wide variety of hardware- and kernel-related
metrics.That will periodically (every 1 second) gather all the metrics of your system. It will
monitor your filesystems, disks, CPUs, memory but also your network statistics,

Kube State Metrics:
kube-state-metrics is a service that listens to the Kubernetes API server and generates metrics
about the state of the objects, including deployments, nodes, and pods.ls like Prometheus and Grafana.

Grafana
-------
Grafana is designed for analyzing and visualizing metrics such as system CPU, memory, disk and
I/O utilization.
Grafana allows you to query, visualize, alert on and understand your metrics no matter where
they are stored. Create, explore, and share dashboards with your team and foster a data driven
culture:

Prerequisite:
1) Kubernetes Cluster With Storage Class Configured(Dynamic Provisioning).
If Storage Class is not Configured then make persistence enabled as false in helm
values files # Which is not suggestable as if pods are terminated and recreated we
will lost the data.
2) Kubernetes nodes with minimum 4GB RAM With 2 Core Processer.
3) Server which has kubectl & Helm Configured.
