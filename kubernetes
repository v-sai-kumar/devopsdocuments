service can be accessed only with in pod

pods will have unique ip address given by 

network namespace can be found under var/run/netns

CRI plugin creates a network names0pace 

when we not sepcify any namespace its used default namespace

within namespace object name should be unique 

we can use same object name for differnet namespaces

we have to carefully give selectors and labels for the specific pod to access or else we get error 

to enter inside container from pod we use --> kubectl exec -it podname -n namespacename-ns -- sh
in  the above if we have more continers in the pod it will exec by index value 0th index first

if we have more contianers in the pod we use contianer name -->kubectl exec -it podname -c contianername -n namespacename-ns -- sh

we communicate b/w pods within node by using localnetwork or pod ip 

service will act as a loadbalancer and service discovery 
 *cluster port we can access within cluster only 
 *nodeport we can access from outside cluster webpage 30000 to 32767 node port range

if we have 2 pods first pod will be executed if we execute pod 

if we want to execute specific container in a pod we have to give container name 
eg - kubectl exec -it podname -c containername bash 

curl -v servicename/containername --> communicates with a web or application serverby specifying a relevant URL and the data that need to be sent or received 

imp:
if 2 pods are in same namespace we communicate by using fqdn (FULLY QULIFIED DOMAIN NAME)
eg: curl -v servicename.namespacename.svc.cluster.local/contianerrname 
curl -vL mavenwebappsvc.test-ns.svc.cluster.local/mavenwebapplication 

Multicontainer pod.yml
we have to give different port numbers so we can use multicontainers
ports:
  - port: 8080
  - targetport: 8080
    name: tomcat
  - port: 80
  - targetport: 80
    name: nginix

if we have different namespace and we should access pods of different namespaces 

NAMESPACE
---------
https://kubernetes.io/docs/concepts/workloads/pods/user-namespaces/

User namespaces is a Linux feature that allows to map users in the container to different users in the host. Furthermore, the capabilities granted to a pod in a user namespace are valid only in the namespace and void outside of it.
The kubelet will pick host UIDs/GIDs a pod is mapped to, and will do so in a way to guarantee that no two pods on the same node use the same mapping.
The valid UIDs/GIDs when this feature is enabled is the range 0-65535. This applies to files and processes (runAsUser, runAsGroup, etc.)
When creating a pod, by default, several new namespaces are used for isolation: a network namespace to isolate the network of the container, a PID namespace to isolate the view of processes, etc. If a user namespace is used, this will isolate the users in the container from the users in the node.

This means containers can run as root and be mapped to a non-root user on the host. Inside the container the process will think it is running as root (and therefore tools like apt, yum, etc. work fine), while in reality the process doesn't have privileges on the host. You can verify this, for example, if you check which user the container process is running by executing ps aux from the host. The user ps shows is not the same as the user you see if you execute inside the container the command id.

Furthermore, as users on each pod will be mapped to different non-overlapping users in the host, it is limited what they can do to other pods too.


PODLIFECYCLE
-----------
https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/

Pods follow a defined lifecycle, starting in the Pending phase, moving through Running if at least one of its primary containers starts OK, and then through either the Succeeded or Failed phases depending on whether any container in the Pod terminated in failure.

Whilst a Pod is running, the kubelet is able to restart containers to handle some kind of faults. Within a Pod, Kubernetes tracks different container states and determines what action to take to make the Pod healthy again


DEPLOYMNENTS
------------
https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.



CONTROLL PROBES
---------------
https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/



probe means health check 
Types of probe
The kubelet can optionally perform and react to three kinds of probes on running containers:

livenessProbe
Indicates whether the container is running. If the liveness probe fails, the kubelet kills the container, and the container is subjected to its restart policy. If a container does not provide a liveness probe, the default state is Success.

2 types we have (Define a liveness command  / Define a liveness HTTP request)
--> Define a liveness command 
 livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5

kubectl get pod liveness-exec 

--> Define a liveness HTTP request
Another kind of liveness probe uses an HTTP GET request 

livenessProbe:
      httpGet:
        path: /healthz
        port: 8080
initialDelaySeconds: 5
periodSeconds: 5

kubectl describe pod liveness-http  --> to get the http request liveness

Any code greater than or equal to 200 and less than 400 indicates success. Any other code indicates failure.

--> Define a TCP liveness probeDefine a TCP liveness probe
A third type of liveness probe uses a TCP socket. With this configuration, the kubelet will attempt to open a socket to your container on the specified port. If it can establish a connection, the container is considered healthy, if it can't it is considered a failure. 

readinessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 10
    livenessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 10

--> Use a named port
You can use a named port for HTTP and TCP probes. gRPC probes do not support named ports.

For example:

ports:
- name: liveness-port
  containerPort: 8080

livenessProbe:
  httpGet:
    path: /healthz
    port: liveness-port


readinessProbe
Indicates whether the container is ready to respond to requests. If the readiness probe fails, the endpoints controller removes the Pod's IP address from the endpoints of all Services that match the Pod. The default state of readiness before the initial delay is Failure. If a container does not provide a readiness probe, the default state is Success.

 ReadynessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
The periodSeconds field specifies that the kubelet should perform a liveness probe every 5 seconds. The initialDelaySeconds field tells the kubelet that it should wait 5 seconds before performing the first probe. To perform a probe, the kubelet executes the command cat /tmp/healthy in the target container. If the command succeeds, it returns 0, and the kubelet considers the container to be alive and healthy. If the command returns a non-zero value, the kubelet kills the container and restarts it.


startupProbe
Indicates whether the application within the container is started. All other probes are disabled if a startup probe is provided, until it succeeds. If the startup probe fails, the kubelet kills the container, and the container is subjected to its restart policy. If a container does not provide a startup probe, the default state is Success.

HPA
----
https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/

https://kubernetes.io/docs/tutorials/kubernetes-basics/scale/scale-intro/

A HorizontalPodAutoscaler (HPA for short) automatically updates a workload resource (such as a Deployment or StatefulSet), with the aim of automatically scaling the workload to match demand.

Horizontal scaling means that the response to increased load is to deploy more Pods. This is different from vertical scaling, which for Kubernetes would mean assigning more resources (for example: memory or CPU)
to the Pods that are already running for the workload.

If the load decreases, and the number of Pods is above the configured minimum, the HorizontalPodAutoscaler instructs the workload resource (the Deployment, StatefulSet, or other similar resource) to scale back down.

he HPA controller will increase and decrease the number of replicas (by updating the Deployment) to maintain an average CPU utilization across all Pods of 50%. The Deployment then updates the ReplicaSet 
- this is part of how all Deployments work in Kubernetes - and then the ReplicaSet either adds or removes Pods based on the change to its .spec.

kubernetes we will create normally deplyment and attach HPA  to the deplyment we will use HPA scaletargetref (kind , name) to the deployment section same kind and name should match then only HPA will 
work as expected
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10

volumes
-------
https://kubernetes.io/docs/concepts/storage/volumes/

On-disk files in a container are ephemeral (lasting for a very short time)
which presents some problems for non-trivial applications when running in containers. One problem occurs when a container crashes or is stopped. Container state is not saved so all of the files that were created or modified during the lifetime of the container are lost. During a crash, kubelet restarts the container with a clean state. Another problem occurs when multiple containers are running in a Pod and need to share files. It can be challenging to setup and access a shared filesystem across all of the containers. The Kubernetes volume abstraction solves both of these problems.

Volumes cannot mount within other volumes (but see Using subPath for a related mechanism). Also, a volume cannot contain a hard link to anything in a different volume

Kubernetes supports many types of volumes. A Pod can use any number of volume types simultaneously. Ephemeral volume types have a lifetime of a pod, but persistent volumes exist beyond the lifetime of a pod. 

volumes:
  - name: example-volume
    # mount /data/foo, but only if that directory already exists
    hostPath:
      path: /data/foo # directory location on host
      type: Directory # this field is optional
TYPES OF VOLUMES
----------------
Kubernetes supports several types of volumes. 
Like
emptydir -->
--------
For a Pod that defines an emptyDir volume, the volume is created when the Pod is assigned to a node. As the name says, the emptyDir volume is initially empty. All containers in the Pod can read and write the same files in the emptyDir volume, though that volume can be mounted at the same or different paths in each container. When a Pod is removed from a node for any reason, the data in the emptyDir is deleted permanently.

eg:
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: registry.k8s.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir:
      sizeLimit: 500Mi

configmap 
hostpath-->
--------
A hostPath volume mounts a file or directory from the host node's filesystem into your Pod. This is not something that most Pods will need, but it offers a powerful escape hatch for some applications.
Warning:
Using the hostPath volume type presents many security risks. If you can avoid using a hostPath volume, you should. For example, define a local PersistentVolume, and use that instead.
Take care when using hostPath volumes, whether these are mounted as read-only or as read-write, because:

Access to the host filesystem can expose privileged system credentials (such as for the kubelet) or privileged APIs (such as the container runtime socket), that can be used for container escape or to attack other parts of the cluster.
Pods with identical configuration (such as created from a PodTemplate) may behave differently on different nodes due to different files on the nodes.

Some uses for a hostPath are:
unning a container that needs access to node-level system components (such as a container that transfers system logs to a central location, accessing those logs using a read-only mount of /var/log)
making a configuration file stored on the host system available read-only to a static pod; unlike normal Pods, static Pods cannot access ConfigMaps

---
# This manifest mounts /data/foo on the host as /foo inside the
# single container that runs within the hostpath-example-linux Pod.
#
# The mount into the container is read-only.
apiVersion: v1
kind: Pod
metadata:
  name: hostpath-example-linux
spec:
  os: { name: linux }
  nodeSelector:
    kubernetes.io/os: linux
  containers:
  - name: example-container
    image: registry.k8s.io/test-webserver
    volumeMounts:
    - mountPath: /foo
      name: example-volume
      readOnly: true
  volumes:
  - name: example-volume
    # mount /data/foo, but only if that directory already exists
    hostPath:
      path: /data/foo # directory location on host
      type: Directory # this field is optional


Local --> A local volume represents a mounted local storage device such as a disk, partition or directory.
------
Local volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported.

Compared to hostPath volumes, local volumes are used in a durable and portable manner without manually scheduling pods to nodes. The system is aware of the volume's node constraints by looking at the node affinity on the PersistentVolume.

However, local volumes are subject to the availability of the underlying node and are not suitable for all applications. If a node becomes unhealthy, then the local volume becomes inaccessible by the pod. The pod using this volume is unable to run. Applications using local volumes must be able to tolerate this reduced availability, as well as potential data loss, depending on the durability characteristics of the underlying disk.

NFS
----
An nfs volume allows an existing NFS (Network File System) share to be mounted into a Pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of an nfs volume are preserved and the volume is merely unmounted. This means that an NFS volume can be pre-populated with data, and that data can be shared between pods. NFS can be mounted by multiple writers simultaneously.

Note:
You must have your own NFS server running with the share exported before you can use it.

Also note that you can't specify NFS mount options in a Pod spec. You can either set mount options server-side or use /etc/nfsmount.conf. You can also mount NFS volumes via PersistentVolumes which do allow you to set mount options.

eg:
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: registry.k8s.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /my-nfs-data
      name: test-volume
  volumes:
  - name: test-volume
    nfs:
      server: my-nfs-server.example.com
      path: /my-nfs-volume
      readOnly: true

secret
-------
A secret volume is used to pass sensitive information, such as passwords, to Pods. You can store secrets in the Kubernetes API and mount them as files for use by pods without coupling to Kubernetes directly. secret volumes are backed by tmpfs (a RAM-backed filesystem) so they are never written to non-volatile storage.

azurefilesystem
cephfs

presistent - volumes
--------------------
https://kubernetes.io/docs/concepts/storage/persistent-volumes/

A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. It is a resource in the cluster just like a node is a cluster resource. PVs are volume plugins like Volumes

A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany, ReadWriteMany, or ReadWriteOncePod)

Provisioning
-------------
There are two ways PVs may be provisioned: statically or dynamically.

//Static
A cluster administrator creates a number of PVs. They carry the details of the real storage, which is available for use by cluster users. They exist in the Kubernetes API and are available for consumption.

//Dynamic
When none of the static PVs the administrator created match a user's PersistentVolumeClaim, the cluster may try to dynamically provision a volume specially for the PVC. This provisioning is based on StorageClasses: the PVC must request a storage class and the administrator must have created and configured that class for dynamic provisioning to occur. Claims that request the class "" effectively disable dynamic provisioning for themselves.

To enable dynamic storage provisioning based on storage class, the cluster administrator needs to enable the DefaultStorageClass admission controller on the API server. This can be done, for example, by ensuring that DefaultStorageClass is among the comma-delimited, ordered list of values for the --enable-admission-plugins flag of the API server component. For more information on API server command-line flags, check kube-apiserver documentation.

//Binding
A user creates, or in the case of dynamic provisioning, has already created, a PersistentVolumeClaim with a specific amount of storage requested and with certain access modes. A control loop in the control plane watches for new PVCs, finds a matching PV (if possible), and binds them together. If a PV was dynamically provisioned for a new PVC, the loop will always bind that PV to the PVC. Otherwise, the user will always get at least what they asked for, but the volume may be in excess of what was requested. Once bound, PersistentVolumeClaim binds are exclusive, regardless of how they were bound. A PVC to PV binding is a one-to-one mapping, using a ClaimRef which is a bi-directional binding between the PersistentVolume and the PersistentVolumeClaim.

Claims will remain unbound indefinitely if a matching volume does not exist. Claims will be bound as matching volumes become available. For example, a cluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi. The PVC can be bound when a 100Gi PV is added to the cluster.

Config Maps
------------
A ConfigMap provides a way to inject configuration data into pods. The data stored in a ConfigMap can be referenced in a volume of type configMap and then consumed by containerized applications running in a pod.

When referencing a ConfigMap, you provide the name of the ConfigMap in the volume. You can customize the path to use for a specific entry in the ConfigMap. The following configuration shows how to mount the log-config ConfigMap onto a Pod called configmap-pod:

apiVersion: v1
kind: Pod
metadata:
  name: configmap-pod
spec:
  containers:
    - name: test
      image: busybox:1.28
      command: ['sh', '-c', 'echo "The app is running!" && tail -f /dev/null']
      volumeMounts:
        - name: config-vol
          mountPath: /etc/config
  volumes:
    - name: config-vol
      configMap:
        name: log-config
        items:
          - key: log_level
            path: log_level

The log-config ConfigMap is mounted as a volume, and all contents stored in its log_level entry are mounted into the Pod at path /etc/config/log_level. Note that this path is derived from the volume's mountPath and the path keyed with log_level.

kubectl get events -n namespacename 
to see events for the specific namespace like logs

livenessprobe and readynessprobe we have to use mandatory to know the application status running  and  responding to recive the traffic for that application 

Pull an Image from a Private Registry
--------------------------------------
refer offical link
https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ 

 create a Pod that uses a Secret to pull an image from a private container image registry or repository. There are many private registries in use
https://index.docker.io/v1/  

Create this Secret, naming it regcred:

we should add in container specifications field it will be applicable for only that namespace 
 imagePullSecrets:
  - name: regcred

kubectl create secret docker-registry regcred --docker-server=<your-registry-server> --docker-username=<your-name> --docker-password=<your-pword> --docker-email=<your-email>

where:
<your-registry-server> is your Private Docker Registry FQDN. Use https://index.docker.io/v1/ for DockerHub.
<your-name> is your Docker username.
<your-pword> is your Docker password.
<your-email> is your Docker email.
You have successfully set your Docker credentials in the cluster as a Secret called regcred.

To get the output from the secret
kubectl get secret regcred --output=yaml  


Note: To use image pull secrets for a Pod (or a Deployment, or other object that has a pod template that you are using), you need to make sure that the appropriate Secret does exist in the right namespace. The namespace to use is the same namespace where you defined the Pod.


Another private registry EKS 
AWS EKS

we will create a IAM role and assign policy for that ec2 instance Readonlypolicy assign and give 
we can see now based on policy that server will autoamtocally fetch the private image from the eks registry and we will not require any secret to use for the authentication wile comes to AWS cloud

stateful sets 
-------------
"A StatefulSet is a Kubernetes controller that manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods. It's particularly useful for applications that require stable, unique network identities and storage for each of their components.

StatefulSets are typically used for stateful applications, such as databases, where each instance needs a stable, unique identity and the ability to handle persistent data. They are designed to maintain the identity of the Pods, even when they are rescheduled, and to ensure that each Pod has a unique hostname and DNS name.

Here are some key features of StatefulSets:

Stable, unique network identities: Each Pod in a StatefulSet has a unique, stable hostname and DNS name, allowing other services to reliably discover and connect to them.

Persistent storage: StatefulSets support the use of Persistent Volumes (PVs) and Persistent Volume Claims (PVCs) to provide each Pod with stable, persistent storage.

Ordered deployment and scaling: StatefulSets deploy and scale their Pods in a specific order, ensuring that each Pod is fully up and running before the next one is started.

Ordered deletion: When a StatefulSet is deleted, its Pods are terminated in reverse order, ensuring that each Pod has a chance to cleanly shut down and release any resources it may be holding.

Rolling updates: StatefulSets support rolling updates, allowing you to update the Pods in a controlled, predictable manner.

To use a StatefulSet, you define a StatefulSet object in your Kubernetes cluster, specifying the number of replicas, the container image, and any other necessary configuration options. The Kubernetes control plane will then create and manage the Pods according to your specifications.

In summary, StatefulSets are a powerful tool for deploying and managing stateful applications in Kubernetes. By providing stable, unique network identities and persistent storage for each Pod, they enable you to run complex, stateful applications with confidence and ease."














